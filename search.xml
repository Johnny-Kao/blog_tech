<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Google Analytics(GA)排除本机流量的 5 种方式]]></title>
    <url>%2FGoogle%20Analytics(GA)%E6%8E%92%E9%99%A4%E6%9C%AC%E6%9C%BA%E6%B5%81%E9%87%8F%E7%9A%845%E7%A7%8D%E6%96%B9%E5%BC%8F.html</url>
    <content type="text"><![CDATA[Google Analytics(GA)其实已经内建流量过滤的功能，但是在实际使用上还是有些不方便，这边提供教学教你如何过滤掉自己的浏览数据。 为了避免数据不准确，一开始设定Google Analytics的时候，就必须将自己及公司同事的浏览数据过滤掉。 如何设定GA本身就已经内建设定工具，你可以从左下角的【设定】》【账户】》【所有筛选器】进入设定页面，根据浏览来源不同： 来自ISP网域的流量 来自IP位址的流量 子目录获得的流量 主机名称获得的流量 设定过滤非目标流量。 实际问题但是如果电脑是浮动IP，也就是说IP会随着时间而改变，那么即使设定了过滤器，基本上也是徒劳无功，该如何解决？ 解决方式解决方式有 5 个： 使用运算规则式 使用外褂 Google Analytics (分析) 不透露資訊外掛程式 IT部门屏蔽GA流量 调整GA代码 设定本地Host 第 1 种方式对于一般人有困难，但是比较有趣的是，它可以过滤到ipv6的流量，但是ipv6使用的人数不多，可能未来这个功能才会派上用场。 第 2 种方式对于个人来说是最方便的，直接安装即可，这样就会过滤掉自己的流量了。如果你没有编程能力，建议直接选这个吧。 但是如果公司是浮动流量的话，那么你只能拜托大家一个一个安装了。 第 3 种方式如果公司有IT部门，而且大家是共用WIFI的话，可以直接在WIFI路由器设定Host，屏蔽掉GA的流量。这种方式最简单，但是前提是你的IT部门必须知道他要做什么（不要怀疑，很多IT部门不知道这个东西） 第 4 种方式我觉得是最酷的方式，但是并非屏蔽所有流量，只屏蔽了网站在本地测试时候的流量。换句话说，如果网站上线后，你自己浏览的行为数据也会记录。 如果你是一个跟工程师关系很好的产品经理（虽然工程师都想要杀产品祭天？）那么极度推荐这个方式。 请他们添加代码的时候，判断是否为本地流量后，排除统计 1234567&lt;script&gt;var host &#x3D; window.location.hostname;if(host !&#x3D; &quot;localhost&quot;)&#123; &#x2F;&#x2F; your google analytic code here&#125;&lt;&#x2F;script&gt; 第 5 种方式适合工程师，直接修改本地Host文件，屏蔽GA浏览，以MAC设定为例： 123456789# 修改 Host文件jk@local:~$ vi &#x2F;etc&#x2F;hosts# 添加一下代码后，存档关闭# My filters127.0.0.1 www.google-analytics.com127.0.0.1 google-analytics.com127.0.0.1 ssl.google-analytics.com 这种方式好处是你不必安装插件即可完成。 后记介绍了那么多方式，想要简单设定的话，直接选第 2 种安装插件的方式吧！ 点击安装》Google Analytics (分析) 不透露資訊外掛程式 如何掌握自己的行为数据，一直是一个重要但是大家不关注的议题。自己的行为数据被搜集后，确实可以得到更完善的服务，何必要管理？ 细思恐极，如果你的行为被记录搜集后加以运用，当你要买什么的时候，你可以简单获得，当你不要买什么的时候，可以透过长时间的广告投放，让你逐渐改变喜好，最后决定购买。 这样的事情如果发生的话，你不觉得非常可怕吗？ 累计阅读量次]]></content>
      <categories>
        <category>SEO学习</category>
      </categories>
      <tags>
        <tag>SEO</tag>
        <tag>Google Analytics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[购买记忆卡翻车记录]]></title>
    <url>%2F%E8%B4%AD%E4%B9%B0%E8%AE%B0%E5%BF%86%E5%8D%A1%E7%BF%BB%E8%BD%A6%E8%AE%B0%E5%BD%95.html</url>
    <content type="text"><![CDATA[因为想要改装iPod Classic的缘故，上网购买了一张Microsd的记忆卡.没想到在9012年居然还有扩容卡的存在!记录一下这次的检测记录，并且提供检测方式供大家参考。 什么是扩容卡扩容卡是指用非法手段（软件）让容量小的存储卡在电脑上显示出的容量变大（比如8G的显成128G），这些多出来的内存是不能正常使用的，这种卡存入的文件只要超过实际的容量，多出来容量的文件不可用（如音视频损坏，图片打不开，手机安装应用提示应用未安装），数据无故丢失损坏，造成损失。 奇怪的记忆卡没错，就是这次的主角！一张外观 128 GB的小米记忆卡。等等？小米记忆卡？小米也出记忆卡了？基本上就是这点，让我一到手就测试，不测还好… 到手了，苹果电脑Disk Utility 基本看一下，诶？134.21GB？这就是第二个疑点了。电脑的比较大（误？于是乎就开始了这次的检测~ 检测开始本次测试使用了以下2种工具，基本上其他测试软件都已经失效，现在的扩容卡已经能够欺骗测试软件了。 h2testw - Windows 本次的主角 - h2testw，由国外工程师开发非常轻量的软件，点选 Select target，选择自己的记忆卡后（建议先把新的卡片格式化），点击 Write + Verify即可开始测试。 测试中会显示写入了多少测试数据、经过时间（预计花费时间）、目前写入速度等资讯。基本上从这里就可以判断是否是扩容卡了。 从写入速度来看，是否与外观表示的记忆卡规格匹配，速度10MB/s基本上就是扩容卡了。一般扩容卡都会使用廉价卡片，无法有较好的表现。 不必把整张记忆卡写满，非常消耗时间（128GB的卡大约花费5个小时)。一般扩容卡都会使用容量小的卡片，因此可以依据标示容量的 1/2 作为测试值，如记忆卡是128GB，就可以只写入64GB的数据作为参考。 写入一定数据之后，选择Abort跳出。 点击Verify，开始校验数据，如果运气好，应该是什么错误都没发生。运气不好就会出现和上图一样的错误：买到扩容卡了！ 点击下载 》h2testw f3 - Mac OSX这是国外作者写的测试软件 f3，可参考此 Github专案 - F3 这边就不展开教学了。 手动测试扩容卡将原本小容量的卡片，伪装成大容量，那么档案的安全性一定有问题。因此手动也可检测出来。 检测步骤如下： 格式化记忆卡 - exfat, FAT32都可。 准备一个档案3GB左右的档案，重复存入记忆卡，如16GB的，至少可以存入5个相同的档案。 档案命名为file1, file2, file3…. 以此类推。 退出记忆卡，重新插入电脑。 读取刚刚存入的档案，如果是扩容卡，那么一定有档案是无法开启或损坏的。 后记第一次遇到这样的事情，才发现原来记忆卡这种不太获利的产品居然出现这种谋取暴利的野路子。如果真的要购买记忆卡，建议还是从正规渠道购买，毕竟省了钱，丢了数据得不偿失。 累计阅读量次]]></content>
      <categories>
        <category>编程学习</category>
      </categories>
      <tags>
        <tag>Mac</tag>
        <tag>测试</tag>
        <tag>电子产品</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安卓手机模拟点击-探探为例]]></title>
    <url>%2F%E5%AE%89%E5%8D%93%E6%89%8B%E6%9C%BA%E6%A8%A1%E6%8B%9F%E7%82%B9%E5%87%BB-%E6%8E%A2%E6%8E%A2%E4%B8%BA%E4%BE%8B.html</url>
    <content type="text"><![CDATA[本篇文章介绍Adb Shell的一些实际操作，包含获取安卓手机点击坐标、开启安装的APP等，并以探探为例子，将上述技术结合。咱们开始吧！ ADB是什么？ADB的全称为Android Debug Bridge，就是起到调试桥的作用。通过ADB我们可以在Eclipse中方面通过DDMS来调试Android程序。 看不懂对吧？没关系，一句话来说就是：用来操作（调试）android设备的一套指令集。 安装ADB这么好用的工具，首先咱们先安装，此处以MAC OSX为例子 1234567# 通过 Homebrew 安装jk@local:~$ brew cask install android-platform-tools# 安装成功后，测试是否正常jk@local:~$ adb devicesList of devices attached12d6ac34 device # 有设备显示就是正常，如果没显示的话，请百度“开启安卓开发者模式”，先把调试模式打开。 获取点击坐标（16进制）123456789101112131415161718# 进入手机调试终端jk@local:~$ adb shell XXXX:&#x2F; $ # XXXX会显示你的设备名称# 开始获取手机事件，接着点击你想定位的位置，画面会持续跑出很多数据。# 想停止获取可以键盘 Crtl + C 退出XXXX:&#x2F; $ getevent -l&#x2F;dev&#x2F;input&#x2F;event2: EV_ABS ABS_MT_TRACKING_ID 000056f7&#x2F;dev&#x2F;input&#x2F;event2: EV_KEY BTN_TOOL_FINGER DOWN ## 这个是你手指按下屏幕的事件&#x2F;dev&#x2F;input&#x2F;event2: EV_ABS ABS_MT_POSITION_X 0000039c ## 这个是点击的X轴，16进位&#x2F;dev&#x2F;input&#x2F;event2: EV_ABS ABS_MT_POSITION_Y 00000263 ## 这个是点击的Y轴，16进位&#x2F;dev&#x2F;input&#x2F;event2: EV_SYN SYN_REPORT 00000000&#x2F;dev&#x2F;input&#x2F;event2: EV_ABS ABS_MT_TOUCH_MINOR 00000006&#x2F;dev&#x2F;input&#x2F;event2: EV_SYN SYN_REPORT 00000000&#x2F;dev&#x2F;input&#x2F;event2: EV_ABS ABS_MT_TRACKING_ID ffffffff&#x2F;dev&#x2F;input&#x2F;event2: EV_KEY BTN_TOOL_FINGER UP ## 这个是你手指离开屏幕的事件&#x2F;dev&#x2F;input&#x2F;event2: EV_SYN SYN_REPORT 00000000 转换点击坐标（10进制）刚刚获取的坐标为16进制，此时需要转换为10进制方便后续使用。 点击 》 在线装换工具 以刚刚获取的16进制 (x, y) = (0000039c, 00000263)为例，装换后10进制 (x, y) = (924, 611) 模拟ADB点击12# 开始模拟点击，使用转换的10进制坐标jk@local:~$ adb shell input tap 924 611 是不是成功了呢？成功的话，应该就跟你点击的动作是相同的，只是差别在于现在你是以下指令的方式点击。 点击应用 - 探探为例探探交友软件是个非常好的研究对象，首先如画面所示，用户需要点击喜欢或不喜欢来获得配对机会。这个行为就可以透过刚刚学习到的点击来试验，配合上Python的脚本后甚至可以达到自动化的目标。 需要点击喜欢和不喜欢。 分析一下具体思路： 利用Python调用终端指令，进而调用ABD 重复执行点击 控制点击的速度 可参考下列程序码： 12345678910111213141516171819# 引入模组import osimport timeimport random#点击喜欢tap1 &#x3D; &#39;adb shell input tap 725 1785&#39;#点击不喜欢tap2 &#x3D; &#39;adb shell input tap 368 1791&#39;# 重复点击150次for i in range(150): rd &#x3D; random.random() # 设置随机数 if rd &gt;&#x3D; 0.5: # 随机数大于等于0.5时，点击喜欢 os.system(tap1) # 调用ADB指令 else: # 随机数小于0.5时，点击不喜欢 os.system(tap2) # 调用ADB指令 time.sleep(0.03) # 控制点击的速度，300毫秒点击一次 具体成功如上。 后记可改良部分： 执行程序的时候，直接打开探探APP 判断图片后，选择点击喜欢/不喜欢 累计阅读量次]]></content>
      <categories>
        <category>编程学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>安卓</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[黑群晖修改MAC地址]]></title>
    <url>%2F%E9%BB%91%E7%BE%A4%E6%99%96%E4%BF%AE%E6%94%B9MAC%E5%9C%B0%E5%9D%80.html</url>
    <content type="text"><![CDATA[使用自组的黑群晖已经一阵子了，系统是使用网络上常见的DS918二合一引导盘，省去了另外使用U盘的麻烦。 然而唯一美中不足的地方：无法使用常见的网络唤醒功能（WOL），本次教学将一步一步教你如何达成目标。 事前准备 笔记本（苹果，Windows皆可，只要能够使用ssh功能即可） 黑群晖（安装完成，已经可正常使用） 网络（os. 这不是废话？但请注意笔记本和黑群晖必须在同个局域网内） 教学开始ssh连接黑群晖笔记本打开终端机（Terminal），输入下列指令： 1234567891011121314# 注意192.168.xxx.xxx 请修改成自己黑群晖的地址jk@local:~$ ssh admin@192.168.xxx.xxx# 连接成功admin@DS918Plus:~$# 以root执行admin@DS918Plus:~$ sudo -i# 输入密码后回车（不会显示输入的密码）Password:# 以root登入成功root@DS918Plus:~# 挂载开机设定档案123456789101112131415# 建立临时挂载位置root@DS918Plus:~# mkdir &#x2F;tmp&#x2F;boot# 移动到本机设备目录root@DS918Plus:~# cd &#x2F;dev# 挂载开机区，注意指令中的sdb4依据硬件可能为不同root@DS918Plus:&#x2F;dev# mount -t vfat sdb4 &#x2F;tmp&#x2F;boot# 挂载成功后移动到开机区root@DS918Plus:&#x2F;dev# cd &#x2F;tmp&#x2F;boot&#x2F;grub&#x2F;# 查看开机区档案root@DS918Plus:&#x2F;tmp&#x2F;boot&#x2F;grub# lsfonts grub.cfg grubenv loader locale x86_64-efi 修改grub.cfg设定123456789101112131415161718# 修改grub.cfg设定（此处使用vi编辑器，可使用自己习惯的编辑器进行修改）root@DS918Plus:&#x2F;tmp&#x2F;boot&#x2F;grub# vi grub.cfg# 找到文件中与下列相似的内容并修改set sn&#x3D;1780PDN21xxxxset mac1&#x3D;001132xxxxxxset mac2&#x3D;001132xxxxxxset rootdev&#x3D;&#x2F;dev&#x2F;md0# 修改完成范例set sn&#x3D;1780PDN216002 # 通常不需修改set mac1&#x3D;001132e12242 # 单网卡修改此处即可，实体位置可参考主板上实体网卡set mac2&#x3D;001132xxxxxx # 双网卡可修改此处set rootdev&#x3D;&#x2F;dev&#x2F;md0 # 维持不变# 重新启动，完成root@DS918Plus:&#x2F;tmp&#x2F;boot&#x2F;grub# reboot 远端开机在局域网内，可使用任意网络唤醒工具，如手机APP、指令等，此处以python脚本为例，笔记本打开终端机（Terminal），输入下列指令： 12345678910# 安装套件wakeonlanjk@local:~$ pip3 install wakeonlan# 撰写脚本jk@local:~$ python3Python 3.7.6 (default, Dec 30 2019, 19:38:28)[Clang 11.0.0 (clang-1100.0.33.16)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; from wakeonlan import send_magic_packet&gt;&gt;&gt; send_magic_packet(&#39;02.11.32.2A.03.E9&#39;) # 此处MAC地址为黑群晖的地址 完成上述设定之后，即可远端唤醒。 累计阅读量次]]></content>
      <categories>
        <category>编程学习</category>
      </categories>
      <tags>
        <tag>脚本</tag>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac Wifi开关脚本]]></title>
    <url>%2FMac%20Wifi%E5%BC%80%E5%85%B3%E8%84%9A%E6%9C%AC.html</url>
    <content type="text"><![CDATA[上班族在通勤路上使用笔记本电脑的时候，一定会遇到和我相同的问题。 在 Wifi 开启的情形下，只要有新的 Wifi 可连接，就会跳出通知信息。其次就是 Wifi 耗电，使得电脑的续航力下降。 可以使用下列脚本可以快速解决问题，这个脚本将让你可以快速切换 wifi 状态，解放你的双手。 初始设定首先打开终端（terminal）切换到桌面（Desktop）建立脚本，当然你也可以选择在其他地方建立脚本。 12cd ~&#x2F;Desktoptouch wifi.sh 输入完毕后，在桌面会有一个 wifi.sh的档案，请使用文字编辑器（TextEdit）打开，输入下列指令，存档。 1234567#!&#x2F;bin&#x2F;bashif [[ &#96;networksetup -getairportpower en0&#96; &#x3D;&#x3D; *On ]]then networksetup -setairportpower en0 offelse networksetup -setairportpower en0 onfi 切换到刚刚的终端，将文件赋予执行权限，并尝试执行 12chmod +x ~&#x2F;Desktop&#x2F;wifi.sh.&#x2F;wifi.sh 执行成功之后，你会发现你的 wifi 已经关闭了。 日常使用开启终端，输入 ~/Desktop/wifi.sh 之后，就可以使用了。 如果觉得指令太长，可以使用下列方式建立 alias 解决。 123456789vi ~&#x2F;.zshrc # 光标移动到最后一行，输入 i 插入下列指令alias wifi&#x3D;&#39;~&#x2F;Desktop&#x2F;wifi.sh&#39;# 键盘按下 Esc，再输入 :wq! 存档，输入指令生效source ~&#x2F;.zshrc 之后就可以在终端输入 blog 执行指令了，这个脚本如果配上 Alfred 使用后连开启终端的步骤都可以节省了，下回专门撰写一篇教学供大家参考。 累计阅读量次]]></content>
      <categories>
        <category>编程学习</category>
        <category>Bash</category>
      </categories>
      <tags>
        <tag>Mac</tag>
        <tag>脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 新增分类及标签]]></title>
    <url>%2FHexo%20%E6%96%B0%E5%A2%9E%E5%88%86%E7%B1%BB%E5%8F%8A%E6%A0%87%E7%AD%BE.html</url>
    <content type="text"><![CDATA[初次建立 Hexo 静态博客的时候，一直在寻找分类和标签是如何建立的。以前使用 Wordpress 建站的时候，这个功能很直观直接设定即可，现在则必须自行设定。 本篇将教你如何新增分类及标签，并在撰写新文章的时候使用。 创建分类首先在终端 Hexo 根目录建立分类页面 1hexo new page categories 此时会在 /source/categories/ 目录下生成 index.md 档案。 上面的这个操作是利用 Hexo 创建一个页面，该页面是以 page 的形式展现。平时我们写博客新增文章的时候，则是以 post 的形式新增文章。 将档案内容修改如下： 12345678910111213---# 这个是页面展示的时候的标题，可自行修改title: 文章分类 # 时间自动生成，不必理会date: 2019-09-20 20:33:35 # 填写 categories 即可type: categories # 如果有使用评论插件（如：Disqus），选择 false 后，该页面不会展示评论插件。 comments: false --- 上面设定完之后，基本上功能已经开通完成。那么如何在博客上显示页面呢？这边以 Next 主题为例子。 显示分类页面修改 Hexo Next主题的设定文件，路径为 theme/next/_config.yml，找到 menu 添加 categories 项目即可。 123menu: tags: &#x2F;tags&#x2F; || tags categories: &#x2F;categories&#x2F; || th 当然别忘了修改后存档，重新生成静态博客. 12hexo generatehexo deloy 完成后效果 文字撰写添加分类新增完分类后，撰写文章的时候就可以使用了 撰写文章的时候，只要加入 categories: 即可。添加的方式有 2 种，可以参考下列范例，当文章属于多个分类的时候使用方式一，单一分类的时候使用方式二。 12345678910111213---title: Hexo 新增分类及标签# 方式一categories: - 编程学习 - Hexo# 方式二 categories: 编程学习date: 2019-09-24 20:15:40--- 具体效果可参考本篇文章~ 创建标签首先在终端 Hexo 根目录建立分类页面 1hexo new page tags 此时会在 /source/tags/ 目录下生成 index.md 档案。 上面的这个操作是利用 Hexo 创建一个页面，该页面是以 page 的形式展现。平时我们写博客新增文章的时候，则是以 post 的形式新增文章。 将档案内容修改如下： 12345678910111213---# 这个是页面展示的时候的标题，可自行修改title: 文章分类 # 时间自动生成，不必理会date: 2019-09-20 20:33:35 # 填写 tags 即可type: tags # 如果有使用评论插件（如：Disqus），选择 false 后，该页面不会展示评论插件。 comments: false --- 上面设定完之后，基本上功能已经开通完成。那么如何在博客上显示页面呢？这边以 Next 主题为例子。 显示标签页面修改 Hexo Next主题的设定文件，路径为 theme/next/_config.yml，找到 menu 添加 tags 项目即可。 123menu: tags: &#x2F;tags&#x2F; || tags categories: &#x2F;categories&#x2F; || th 当然别忘了修改后存档，重新生成静态博客. 12hexo generatehexo deloy 完成后效果 文字撰写添加标签新增完标签后，撰写文章的时候就可以使用了 撰写文章的时候，只要加入 tags: 即可。添加的方式有 2 种，可以参考下列范例，当文章属于多个标签的时候使用方式一，单一标签的时候使用方式二。 12345678910111213---title: Hexo 新增分类及标签# 方式一tags: - 编程学习 - Hexo# 方式二 tags: 编程学习date: 2019-09-24 20:15:40--- 具体效果可参考本篇文章~ 有没有发现无论是分类还是标签，两种添加的方式机会相同？ 没错，这个就是 Hexo 的美妙，另外其实常见的 404 错误页面，也可以用类似的方式生成哦。下次再撰写一篇来详细解说吧。 累计阅读量次]]></content>
      <categories>
        <category>编程学习</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo建立个人主页]]></title>
    <url>%2Fhexo-create-personal-homepage.html</url>
    <content type="text"><![CDATA[初次使用 Hexo 建立完静态博客的时候，默认的首页就是博客文章，本篇文章教你如何将首页设定成自己的静态页面。 达成效果当初始化 Hexo 静态博客部署之后，输入网址之后（你的github账号.github.io）通常你会直接进入博客主页。 但是，如果我希望优先展示个人介绍页面、个人作品集或是落地页的话，该如何处理呢？ 先进入落地页： 点击 Blog 之后才进入博客： 准备内容 静态页面 Hexo 站点设定档（_config.yml） 具体操作修改 Hexo 站点设定档找到站点设定档案，一般在 Hexo 根目录下，进行修改。 123456url: http:&#x2F;&#x2F;indeedinvest.info&#x2F;blog&#x2F; # 输入你的网址root: &#x2F;blog&#x2F; # 修改成 &#x2F;blog&#x2F;# Directorysource_dir: sourcepublic_dir: public&#x2F;blog&#x2F; # 修改成 public&#x2F;blog&#x2F; 重新生成 Hexo在终端 Hexo 根目录路径下输入下列指令，重新生成静态博客 12hexo clean # 清除旧档案hexo generate # 重新生成 确认静态页面档案一个静态页面至少包含 index.html 档案。我的静态页面包含的档案清单如下： 将所有的档案复制到 Hexo 目录下的 public 资料夹，复制成功之后如图 部署上线经过上述的操作后，你已经可以重新部署。 12hexo generate # 生成档案hexo deploy # 部署上线 是不是很简单啊？完成上述的操作之后，进入自己的页面只要输入自己的网址即可。 想要进入博客页面，输入自己的网址/blog/即可。 这样的设定能够让你轻松展示自己的作品集哦。 累计阅读量次]]></content>
      <categories>
        <category>编程学习</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas 常用基础教学]]></title>
    <url>%2FPandas%20%E5%B8%B8%E7%94%A8%E5%9F%BA%E7%A1%80%E6%95%99%E5%AD%A6.html</url>
    <content type="text"><![CDATA[Pandas 是 Python 中用于处理数据分析的一个套件，提供高效能、简易使用的资料格式（Data Frame）让使用者可以快速操作及分析资料。 一般用于金融数据处理及时间序列分析，本篇将介绍几个常见的使用方式，供读者参考。 导入 Pandas12345# 导入pandas套件import pandas as pd# 如果没安装的话，可在终端输入指令安装(Python3)pip3 install pandas 导入数据以 CSV 为例，导入数据常用的参数如下： 范例文件下载 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748df = pd.read_csv( # 文件数据路径，必填 '~/Desktop/demo.csv', # 数据分隔符，csv文件默认为逗号，选填 sep= ',', # 跳过数据文件第 1 行不读 # skiprows = 1, # 只读前 15 行，选填，默认读取全部数据 nrows = 15, # 指定列为日期格式，选填，默认全部格式为字串 parse_dates = ['OrderDate'], # 指定列设为index，选填，默认 index 为 0，1，2 index_col = ['OrderDate'], # 读取指定的列数据，选填，默认读取全部数据 usecols = ['OrderDate', 'Region', 'Rep'], # 读取数据跳过报错数据，只载入正确数据，用于脏数据较多的场景 error_bad_lines = False, # 将数据中的null识别为空值 na_values = 'NULL')# 打印结果print(df) Region Rep OrderDate 2018-01-06 East Jones 2018-01-23 Central Kivell 2018-02-09 Central Jardine 2018-02-26 Central Gill 2018-03-15 West Sorvino 2018-04-01 East Jones 2018-04-18 Central Andrews 2018-05-05 Central Jardine 2018-05-22 West Thompson 2018-06-08 East Jones 2018-06-25 Central Morgan 2018-07-12 East Howard 2018-07-29 East Parent 2018-08-15 East Jones 2018-09-01 Central Smith 数据概览12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 数据行列print(df.shape) (15, 2)# 数据列名称 print(df.columns) Index(['Region', 'Rep'], dtype='object')# 数据行名称print(df.index) DatetimeIndex(['2018-01-06', '2018-01-23', '2018-02-09', '2018-02-26', '2018-03-15', '2018-04-01', '2018-04-18', '2018-05-05', '2018-05-22', '2018-06-08', '2018-06-25', '2018-07-12', '2018-07-29', '2018-08-15', '2018-09-01'], dtype='datetime64[ns]', name='OrderDate', freq=None)# 数据列属性print(df.dtypes) Region object Rep object dtype: object# 数据前 5 行数据print(df.head(5)) Region Rep OrderDate 2018-01-06 East Jones 2018-01-23 Central Kivell 2018-02-09 Central Jardine 2018-02-26 Central Gill 2018-03-15 West Sorvino# 数据后 5 行数据print(df.tail(5)) Region Rep OrderDate 2018-06-25 Central Morgan 2018-07-12 East Howard 2018-07-29 East Parent 2018-08-15 East Jones 2018-09-01 Central Smith# 数据描述 - 用于快速了解数字类型的数据描述print(df.describe()) Region Rep count 15 15 unique 3 11 top Central Jones freq 7 4 数据读取 - 列123456789101112131415161718192021222324252627282930313233343536373839# 根据列名选取数据，类型为Seriesprint(df['Rep']) OrderDate 2018-01-06 Jones 2018-01-23 Kivell 2018-02-09 Jardine 2018-02-26 Gill 2018-03-15 Sorvino 2018-04-01 Jones 2018-04-18 Andrews 2018-05-05 Jardine 2018-05-22 Thompson 2018-06-08 Jones 2018-06-25 Morgan 2018-07-12 Howard 2018-07-29 Parent 2018-08-15 Jones 2018-09-01 Smith Name: Rep, dtype: object # 选取多列，类型为DataFrameprint(df[['Region', 'Rep']]) Region Rep OrderDate 2018-01-06 East Jones 2018-01-23 Central Kivell 2018-02-09 Central Jardine 2018-02-26 Central Gill 2018-03-15 West Sorvino 2018-04-01 East Jones 2018-04-18 Central Andrews 2018-05-05 Central Jardine 2018-05-22 West Thompson 2018-06-08 East Jones 2018-06-25 Central Morgan 2018-07-12 East Howard 2018-07-29 East Parent 2018-08-15 East Jones 2018-09-01 Central Smith 数据读取 - 行 - loc12345678910111213141516171819202122232425# 选取指定行，类型为Seriesprint(df.loc['2018-08-15']) Region East Rep Jones Name: 2018-08-15 00:00:00, dtype: object# 选取范围内多行，类型为DataFrameprint(df.loc['2018-04-18':'2018-07-12']) Region Rep OrderDate 2018-04-18 Central Andrews 2018-05-05 Central Jardine 2018-05-22 West Thompson 2018-06-08 East Jones 2018-06-25 Central Morgan 2018-07-12 East Howard# 选取范围内多行、多列，类型为DataFrameprint(df.loc['2018-04-18':'2018-06-08', 'Region':'Rep' ]) Region Rep OrderDate 2018-04-18 Central Andrews 2018-05-05 Central Jardine 2018-05-22 West Thompson 2018-06-08 East Jones 数据读取 - 列 - iloc12345678910111213141516171819202122# 选取某列，类型为Seriesprint(df.iloc[0]) Region East Rep Jones Name: 2018-01-06 00:00:00, dtype: object# 选取范围内列，类型为DataFrameprint(df.iloc[1:3]) # 包含第 1 列，不包含第 3 列，共计显示 2 列 Region Rep OrderDate 2018-01-23 Central Kivell 2018-02-09 Central Jardine # 选取范围内多行、多列，类型为DataFrameprint(df.iloc[0:5,0:1]) # 包含第 0 列，不包含第 5 列，共计显示 4 列，包含第 0 行，不包含第 1 行，共计显示 1 行 Region OrderDate 2018-01-06 East 2018-01-23 Central 2018-02-09 Central 2018-02-26 Central 2018-03-15 West 行列加减乘除123456789101112131415161718192021222324252627282930313233# 字串列加新的字符串print(df['Region'] + '_DEOM') OrderDate 2018-01-06 East_DEOM 2018-01-23 Central_DEOM 2018-02-09 Central_DEOM 2018-02-26 Central_DEOM 2018-03-15 West_DEOM Name: Region, dtype: object# 数字列可直接加减乘除df = pd.read_csv('~/Desktop/demo.csv', nrows = 15, parse_dates = ['OrderDate'], index_col = ['OrderDate']) # 重新载入范例数据print(df['Units'] * 10) OrderDate 2018-01-06 950 2018-01-23 500 2018-02-09 360 2018-02-26 270 2018-03-15 560 Name: Units, dtype: int64# 新增列名称 - Units_x10df['Units_x10'] = df['Units'] * 10df['Units_x10'] OrderDate 2018-01-06 950 2018-01-23 500 2018-02-09 360 2018-02-26 270 2018-03-15 560 2018-04-01 600 Name: Units_x10, dtype: int64 统计函数，累加1234567891011121314151617181920# 求两列的均值print(df[['Units', 'UnitCost']].mean())Units 54.533333UnitCost 15.324000dtype: float64# 求列统计数据，其他参数如 min/std/count/median/count 可自行尝试print(df['Units'].max()) 95# 累加/乘某列，cumsum()/cumprod()df['Units_CUM'] = df['Units'].cumsum()print(df[['Units', 'Units_CUM']]) Units Units_CUM OrderDate 2018-01-06 95 95 2018-01-23 50 145 2018-02-09 36 181 2018-02-26 27 208 2018-03-15 56 264 删除列，列平移1234567891011121314151617181920212223242526272829303132333435363738# Shift平移数据，原数据往下平移 1 行df['Units_Yesterday'] = df['Units'].shift(1)print(df[['Units', 'Units_Yesterday']]) Units Units_Yesterday OrderDate 2018-01-06 95 NaN 2018-01-23 50 95.0 2018-02-09 36 50.0 2018-02-26 27 36.0 2018-03-15 56 27.0 2018-04-01 60 56.0# 删除列方法 1del df['Units_Yesterday']# 删除列方法 2，inplace只是否更新原df数据df.drop(['Units_Yesterday'], axis=1, inplace=True)# 计算行差距，与上 1 行数据差距df['Gap'] = df['Units'].diff(1)print(df[['Units', 'Gap']]) Units Gap OrderDate 2018-01-06 95 NaN 2018-01-23 50 -45.0 2018-02-09 36 -14.0 2018-02-26 27 -9.0 2018-03-15 56 29.0# 计算涨跌幅df['%'] = df['Units'].pct_change(1)print(df[['Units', '%']]) Units % OrderDate 2018-01-06 95 NaN 2018-01-23 50 -0.473684 2018-02-09 36 -0.280000 2018-02-26 27 -0.250000 排名，计数123456789101112131415161718192021222324252627# 排名，ascending决定正序逆序，pct决定排名或排名比例df['Rank'] = df['Total'].rank(ascending = True, pct = False)print(df[['Total', 'Rank']]) Total Rank OrderDate 2018-01-06 189.05 6.0 2018-01-23 999.5 15.0 2018-02-09 179.64 5.0 2018-02-26 539.73 12.0 2018-03-15 167.44 3.0 2018-04-01 299.4 8.0 2018-04-18 149.25 2.0 2018-05-05 449.1 9.5 2018-05-22 63.68 14.0 2018-06-08 539.4 11.0 2018-06-25 449.1 9.5 2018-07-12 57.71 13.0 2018-07-29 1,619.19 1.0 2018-08-15 174.65 4.0 2018-09-01 250 7.0# 计数，统计列中各元素次数，类型为Seriesprint(df['Region'].value_counts()) Central 7 East 6 West 2 Name: Region, dtype: int64 筛选操作123456789101112131415161718192021222324252627282930313233343536373839404142434445# 判断列中是否符合条件，Region列中符合East的结果print(df['Region'] == 'East') OrderDate 2018-01-06 True 2018-01-23 False 2018-02-09 False 2018-02-26 False 2018-03-15 False 2018-04-01 True 2018-04-18 False Name: Region, dtype: bool # 打印出符合的结果数值print(df[df['Region'] == 'East']) Region Rep Units UnitCost Total OrderDate 2018-01-06 East Jones 95 1.99 189.05 2018-04-01 East Jones 60 4.99 299.4 2018-06-08 East Jones 60 8.99 539.4 2018-07-12 East Howard 29 1.99 57.71# 选取多个条件，并展示资料，isin(list)print(df[df['Region'].isin(['East', 'West'])]) Region Rep Units UnitCost Total OrderDate 2018-01-06 East Jones 95 1.99 189.05 2018-03-15 West Sorvino 56 2.99 167.44 2018-04-01 East Jones 60 4.99 299.4 2018-05-22 West Thompson 32 1.99 63.68 2018-06-08 East Jones 60 8.99 539.4 2018-07-12 East Howard 29 1.99 57.71 2018-07-29 East Parent 81 19.99 1,619.19 2018-08-15 East Jones 35 4.99 174.65# 选取index介于两个时间之间的所有数据print(df[(df.index &gt;= '2018-03-15') &amp; (df.index &lt;= '2018-06-25')]) Region Rep Units UnitCost Total OrderDate 2018-03-15 West Sorvino 56 2.99 167.44 2018-04-01 East Jones 60 4.99 299.4 2018-04-18 Central Andrews 75 1.99 149.25 2018-05-05 Central Jardine 90 4.99 449.1 2018-05-22 West Thompson 32 1.99 63.68 2018-06-08 East Jones 60 8.99 539.4 2018-06-25 Central Morgan 90 4.99 449.1 缺失值删除、补齐12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# 找出缺失值df = pd.read_csv('~/Desktop/demo.csv', parse_dates = ['OrderDate'], index_col = ['OrderDate'], error_bad_lines = False, na_values = 'NULL') # 重新载入数据print(df.notnull()) Region Rep Item Units UnitCost Total OrderDate 2018-11-25 True True True True True True 2018-12-12 True True True False True True 2018-12-29 True True True True True True 2019-01-15 True True True True True True 2019-02-01 True True True True False True# 输出无缺失值的行print(df[df.notnull()]) Region Rep Item Units UnitCost TotalOrderDate 2018-01-06 East Jones Pencil 95.0 1.99 189.052018-01-23 Central Kivell Binder 50.0 19.99 999.52018-02-09 Central Jardine Pencil 36.0 4.99 179.642018-02-26 Central Gill Pen 27.0 19.99 539.732018-03-15 West Sorvino Pencil 56.0 2.99 167.44# 删除有空值的行，any指该行有任意空值就删除，all指所有都为空值才删除print(df.dropna(how='any')) Region Rep Item Units UnitCost TotalOrderDate 2018-01-06 East Jones Pencil 95.0 1.99 189.052018-01-23 Central Kivell Binder 50.0 19.99 999.52018-02-09 Central Jardine Pencil 36.0 4.99 179.64# 删除指定列中，有空值的行，subset用来指定可多列print(df.dropna(subset=['UnitCost'], how='any')) Region Rep Item Units UnitCost TotalOrderDate 2018-01-06 East Jones Pencil 95.0 1.99 189.052018-01-23 Central Kivell Binder 50.0 19.99 999.52018-02-09 Central Jardine Pencil 36.0 4.99 179.64# 补齐缺失值，数值来源自定义print(df.fillna(value='我是空白')) Region Rep Item Units UnitCost Total OrderDate 2018-11-08 East Parent Pen 15 19.99 299.85 2018-11-25 Central Kivell Pen Set 96 4.99 479.04 2018-12-12 Central Smith Pencil 我是空白 1.29 86.43 2018-12-29 East Parent Pen Set 74 15.99 1,183.26 2019-01-15 Central Gill Binder 46 8.99 413.54 2019-02-01 Central Smith Binder 87 我是空白 1,305.00 2019-02-18 East Jones Binder 4 4.99 19.96 2019-03-07 West Sorvino Binder 7 19.99 139.93# 补齐缺失值，数值来源为其他栏位df['Units'].fillna(value=df['UnitCost'], inplace=True)print(df[['Units', 'UnitCost']]) # 原本 2018-12-12 的 Units 为空值 Units UnitCostOrderDate 2018-11-08 15.00 19.992018-11-25 96.00 4.992018-12-12 1.29 1.292018-12-29 74.00 15.992019-01-15 46.00 8.99# 补齐缺失值，数值来源为上一个非空值,method = 'ffill'df['Units'].fillna(method='ffill', inplace=True)print(df[['Units', 'UnitCost']])# 补齐缺失值，数值来源为下一个非空值，method = 'bfill'df['Units'].fillna(method='bfill', inplace=True)print(df[['Units', 'UnitCost']])**该主题请多加练习，大部分的数据存在缺少的情形，使用频度相当高** 排序1234567891011# 重新加上 Index，通常用于多个df合并之后df.reset_index(inplace = True)# 重新加上 Index，指定 Index来源df.set_index(['OrderDate'])# 依照指定列排序，acsending决定正序/逆序，1/0print(df.sort_values(by=['OrderDate'], ascending=0))# 依照指定多列排序print(df.sort_values(by=['OrderDate', 'Units'], ascending=[1, 1])) 合并123456789101112131415161718192021222324252627282930313233343536373839404142434445# 先拆分数据df.reset_index(inplace=True)df1 = df.iloc[0:5,0:6] OrderDate Region Rep Item Units UnitCost 0 2018-01-06 East Jones Pencil 95.0 1.99 1 2018-01-23 Central Kivell Binder 50.0 19.99 2 2018-02-09 Central Jardine Pencil 36.0 4.99 3 2018-02-26 Central Gill Pen 27.0 19.99 4 2018-03-15 West Sorvino Pencil 56.0 2.99df2 = df.iloc[3:8,0:6] OrderDate Region Rep Item Units UnitCost 3 2018-02-26 Central Gill Pen 27.0 19.99 4 2018-03-15 West Sorvino Pencil 56.0 2.99 5 2018-04-01 East Jones Binder 60.0 4.99 6 2018-04-18 Central Andrews Pencil 75.0 1.99 7 2018-05-05 Central Jardine Pencil 90.0 4.99# 合并，注意df1 index 3,4 和 df2 index 3,4 两者重复，使用append的时候，不会覆盖print(df1.append(df2)) OrderDate Region Rep Item Units UnitCost 0 2018-01-06 East Jones Pencil 95.0 1.99 1 2018-01-23 Central Kivell Binder 50.0 19.99 2 2018-02-09 Central Jardine Pencil 36.0 4.99 3 2018-02-26 Central Gill Pen 27.0 19.99 4 2018-03-15 West Sorvino Pencil 56.0 2.99 3 2018-02-26 Central Gill Pen 27.0 19.99 4 2018-03-15 West Sorvino Pencil 56.0 2.99 5 2018-04-01 East Jones Binder 60.0 4.99 6 2018-04-18 Central Andrews Pencil 75.0 1.99 7 2018-05-05 Central Jardine Pencil 90.0 4.99# 合并，ignore_index重编indexprint(df1.append(df2, ignore_index = True)) OrderDate Region Rep Item Units UnitCost 0 2018-01-06 East Jones Pencil 95.0 1.99 1 2018-01-23 Central Kivell Binder 50.0 19.99 2 2018-02-09 Central Jardine Pencil 36.0 4.99 3 2018-02-26 Central Gill Pen 27.0 19.99 4 2018-03-15 West Sorvino Pencil 56.0 2.99 5 2018-02-26 Central Gill Pen 27.0 19.99 6 2018-03-15 West Sorvino Pencil 56.0 2.99 7 2018-04-01 East Jones Binder 60.0 4.99 8 2018-04-18 Central Andrews Pencil 75.0 1.99 9 2018-05-05 Central Jardine Pencil 90.0 4.99 去重12345678910111213141516# 建立df3df3 = df1.append(df2, ignore_index = True)# 去重处理df3.drop_duplicates( # 指定根据那些字段为标准，去重；默认全部 subset=['Region', 'Rep', 'Item', 'Units'], # 重复数据如何处理，保留第一条数据，后一条数据，或是一个都不留；first/last/False keep = 'first', # 是否取代原数据 inplace = False )## 改columns名称 字符串处理1234# 字串改为大写print(df['Region'].str.upper())**传统字串处理的方式基本上都可以使用，如len, lower, contains, replace, split等 时间处理12345678910111213141516171819202122232425262728293031323334# 重新载入资料df = pd.read_csv('~/Desktop/demo.csv', sep = ',')# 原数据Orderdate为object形态print(df.dtypes) OrderDate object Region object Rep object Item object Units float64 UnitCost float64 Total object dtype: object# 改为时间变量df['OrderDate'] = pd.to_datetime(df['OrderDate'])# 新数据Orderdate为datetime64[ns] OrderDate datetime64[ns] Region object Rep object Item object Units float64 UnitCost float64 Total object dtype: object# 输出时间的年/月/日/时/分/秒/几周/几天/周天/英文周几，year/month/day/hour/minute/second/week/dayofyear/dayofweek/weekday_namedf['OrderDate'].dt.year**此处很多参数可使用，可参考官方文件**# 增加时间差，Timedelta 增加 1 天print(df['OrderDate'] + pd.Timedelta(days=1)) Rolling、Expanding操作123456789101112131415161718192021# 计算包含该数据，前推 3 个数据的均值，可配合 max, min, std等使用df['R3'] = df['Units'].rolling(3).mean()print(df) OrderDate Region Item Units UnitCost Total R3 0 2018-01-06 East Pencil 95.0 1.99 189.05 NaN 1 2018-01-23 Central Binder 50.0 19.99 999.5 NaN 2 2018-02-09 Central Pencil 36.0 4.99 179.64 60.333333 3 2018-02-26 Central Pen 27.0 19.99 539.73 37.666667 4 2018-03-15 West Pencil 56.0 2.99 167.44 39.666667 5 2018-04-01 East Binder 60.0 4.99 299.4 47.666667# 计算从数据开始到该数据的均值df['Till_Now'] = df['Units'].expanding().mean()print(df) OrderDate Region Item Units UnitCost Total Till_Now 0 2018-01-06 East Pencil 95.0 1.99 189.05 95.000000 1 2018-01-23 Central Binder 50.0 19.99 999.5 72.500000 2 2018-02-09 Central Pencil 36.0 4.99 179.64 60.333333 3 2018-02-26 Central Pen 27.0 19.99 539.73 52.000000 4 2018-03-15 West Pencil 56.0 2.99 167.44 52.800000 5 2018-04-01 East Binder 60.0 4.99 299.4 54.000000 其他12# 修改column名称，字典数据&#123;'原名称':'新名称'&#125;df.rename(columns=&#123;'Region': '区域'&#125;) 输出存档12# 输出成csv档案df.to_csv('output.csv', encoding='gbk', index=False) 累计阅读量次]]></content>
      <categories>
        <category>编程学习</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>资料处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动备份 - Time Machine Rsync]]></title>
    <url>%2F%E8%87%AA%E5%8A%A8%E5%A4%87%E4%BB%BD%20-%20Time%20Machine%20Rsync.html</url>
    <content type="text"><![CDATA[撰写这个脚本前，一直认为自己的档案不重要，平时没有养成备份的习惯。直到电脑的硬盘毁损之后，才发现原来平时日常产生出的点滴，还是值得备份的。 简介本脚本可达成MAC OS系统电脑，定期将本地指定资料夹，备份到远端服务器。 备份方式采用增量备份，使用了 Rsync-time-backup 脚本，可以依照时间点回滚回复，与Timemachine达成同样的备份效果。 系统环境 MAC OS 10.11 以上 Python3.7.0 以上 远端服务器与本机需先配置免密登录SSH 设定方式 复制专案到本地git clone https://github.com/Johnny-Kao/Time-Machine-Mac-Backup.git 复制备份脚本Rsync_tmbackup.sh到本地cp Time-Machine-Mac-Backup/rsync_tmbackup.sh /usr/local/bin/rsync_tmbackup.sh 赋予执行权限chmod +x /usr/local/bin/rsync_tmbackup.sh 设定远端与服务端免密登录Linux SSH免密登录 设定排除备份清单 - 参考项目中档案Exclude.txt格式 设定基本参数 - 修改Backup.py 123456789101112131415161718192021# Log路径&#x2F;名称log_path &#x3D; &#39;YOUR PATH HERE&#39;# WIFI白名单 - 判定是否为指定局域网wifi_list &#x3D; [&#39;YOUR WIFI SSID HERE&#39;, &#39;YOUR WIFI SSID HERE&#39;, &#39;YOUR WIFI SSID HERE&#39;]# 备份主程序路径&#x2F;不备份路径（排除清单）exe_path &#x3D; &#39;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;rsync_tmbackup.sh&#39;exclude_list_path &#x3D; &#39;YOUR EXCLUDE TXT FILE HERE&#39;# 远端服务器ip&#x2F;远端服务器使用者（需要设定远端免密登录）only_sync_in_local &#x3D; &quot;F&quot; # 是否强制局域网更新？ T&#x2F;Fremote_ip_internal &#x3D; &#39;INTERNAL IP - EX. 192.168.0.1&#39; # 局域网IPremote_ip_external &#x3D; &#39;EXTERNAL IP - EX. 143.10.10.10&#39; # 公网IPremote_user &#x3D; &#39;YOUR NAS REMOTE SERVER NAME&#39;# 备份路径[本地，远端]backup_path &#x3D; [ [&#39;LOCAL SOURCE, EX. ~&#x2F;Downloads&#39;,&#39;REMOTE DEST, EX. &#x2F;volume3&#x2F;NAS&#x2F;MacAirBackup&#x2F;Downloads&#x2F;&#39;], [&#39;LOCAL SOURCE, EX. ~&#x2F;Downloads&#39;,&#39;REMOTE DEST, EX. &#x2F;volume3&#x2F;NAS&#x2F;MacAirBackup&#x2F;Downloads&#x2F;&#39;]] 首次执行 python3 Backup.py 首次运行请先参考log档案 123# 你会发现log中含有下列的指令，如果你有10个备份路径，log中就会有10个指令。请依照您的设定，依序在本机执行。ssh -p 22 远端服务器账户@远端服务器IP &#39;mkdir -p -- &quot;&#x2F;volume3&#x2F;NAS&#x2F;MacAirBackup&#x2F;Downloads&quot; ; touch &quot;&#x2F;volume3&#x2F;NAS&#x2F;MacAirBackup&#x2F;Downloads&#x2F;backup.marker&quot;&#39; 添加定期备份功能 crontab -e 待开发清单 复原功能 - 定期、手动 累计阅读量次]]></content>
      <categories>
        <category>编程学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Backup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[關於部落格]]></title>
    <url>%2F%E9%97%9C%E6%96%BC%E9%83%A8%E8%90%BD%E6%A0%BC.html</url>
    <content type="text"><![CDATA[部落格內容包含日常隨筆、日文學習、程序設計及旅遊遊記。透過文章分享經驗，希望讓你少走彎路，熱享生活。 充滿熱情的金融科技從業人員 = 70% 金融從業人員 + 10% 日文學習者 + 10% 投資交易員 + 5% 數據分析師 + 5% 工程師。 累计阅读量次]]></content>
  </entry>
</search>
